{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd0183bbf6827d058c2a2fb0f4acdc0420849dda2b4380af0e437e38c64d798d8b7",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearner:\n",
    "    def __init__(self,\n",
    "                 state_space_parameters, \n",
    "                 epsilon,\n",
    "                 WeightInitializer=None,\n",
    "                 device=None,\n",
    "                 args=None,\n",
    "                 save_path=None,\n",
    "                 state=None,\n",
    "                 qstore=None,\n",
    "                 replaydict = None,\n",
    "                 replay_dictionary = pd.DataFrame(columns=['net',\n",
    "                                                           'spp_size',\n",
    "                                                           'reward',\n",
    "                                                           'epsilon',\n",
    "                                                           'train_flag'])):\n",
    "        self.state_list = []\n",
    "        self.state_space_parameters = state_space_parameters\n",
    "        self.args = args\n",
    "        self.enum = se.StateEnumerator(state_space_parameters, args)\n",
    "        self.stringutils = StateStringUtils(state_space_parameters, args)\n",
    "        self.state = se.State('start', 0, 1, 0, 0, args.patch_size, 0, 0) if not state else state\n",
    "        self.qstore = QValues() \n",
    "        if  type(qstore) is not type(None):\n",
    "            self.qstore.load_q_values(qstore)\n",
    "            self.replay_dictionary = pd.read_csv(replaydict, index_col=0)\n",
    "        else:\n",
    "            self.replay_dictionary = replay_dictionary\n",
    "        self.epsilon = epsilon\n",
    "        self.WeightInitializer = WeightInitializer\n",
    "        self.device = device\n",
    "        self.gpu_mem_0 = GPUMem(torch.device('cuda') == self.device)\n",
    "        self.save_path = save_path\n",
    "        \n",
    "        self.count = args.continue_ite - 1 \n",
    "        # 137 (hard-coded no. for epsilon < 1)\n",
    "\n",
    "    def generate_net(self, epsilon=None, dataset=None):\n",
    "        if epsilon != None:\n",
    "            self.epsilon = epsilon\n",
    "        self.reset_for_new_walk()\n",
    "        state_list = self.run_agent()\n",
    "\n",
    "        net_string = self.stringutils.state_list_to_string(\n",
    "            state_list, num_classes=len(dataset.val_loader.dataset.class_to_idx))\n",
    "\n",
    "        train_flag = True\n",
    "        if net_string in self.replay_dictionary['net'].values:\n",
    "            spp_size = self.replay_dictionary[self.replay_dictionary['net']\n",
    "                                              == net_string['spp_size'].values[0]]\n",
    "\n",
    "    def reset_for_new_walk(self):\n",
    "\n",
    "        self.state_list = []\n",
    "        self.state = se.State('start', 0, 1, 0, 0, self.args.patch_size, 0, 0)\n",
    "\n",
    "    def run_agent(self):\n",
    "        while self.state.terminate == 0:\n",
    "            self.transition_q_learning()\n",
    "        return self.state_list\n",
    "\n",
    "    def transition_q_learning(self):\n",
    "        if self.state.as_tuple() not in self.qstore.q:\n",
    "            self.enum.enumerate_state(self.state, self.qstore.q)        \n",
    "        action_values = self.qstore.q[self.state.as_tuple()]\n",
    "        if np.random.random() < self.epsilon:\n",
    "            action = se.State(\n",
    "                state_list=action_values['actions'][np.random.randint(\n",
    "                    len(action_values['actions']))])\n",
    "        else:\n",
    "            max_q_value = max(action_values['utilities'])\n",
    "            max_q_indexes = [i for i in range(len(action_values['actions'])) \n",
    "                if action_values['utilities'][i] == max_q_value]\n",
    "            max_actions = [action_values['actions'][i] for i in max_q_indexes]\n",
    "            action = se.State(state_list=max_actions[np.random.randint(len(max_actions))])\n",
    "\n",
    "        self.state = self.enum.state_action_transition(self.state, action)\n",
    "        self.__post_transition_updates()\n",
    "\n",
    "    def post_transition_updates(self):\n",
    "        non_bucketed_state = self.state.copy()\n",
    "        self.state_list.append(non_bucketed_state)\n",
    "\n",
    "    def sample_replay_for_update(self):\n",
    "        net = self.replay_dictionary.iloc[-1]['net']\n",
    "        reward_best_val = self.replay_dictionary.iloc[-1]['reward']\n",
    "        train_flag = self.replay_dictionary.iloc[-1]['train_flag']\n",
    "        state_list = self.stringutils.convert_model_string_to_states(cnn_parse('net', net))\n",
    "        # if train_flag:\n",
    "        self.__update_q_value_sequence(state_list, self.__accuracy_to_reward(reward_best_val/100.))\n",
    "\n",
    "        for i in range(self.state_space_parameters.replay_number-1):\n",
    "            net = np.random.choice(self.replay_dictionary['net'])\n",
    "            reward_best_val = self.replay_dictionary[self.replay_dictionary['net'] == net]['reward'].values[0]\n",
    "            train_flag = self.replay_dictionary[self.replay_dictionary['net'] == net]['train_flag'].values[0]\n",
    "            state_list = self.stringutils.convert_model_string_to_states(cnn_parse('net', net))\n",
    "            # if train_flag == True:\n",
    "            self.__update_q_value_sequence(state_list, self.__accuracy_to_reward(reward_best_val/100.))            \n",
    "\n",
    "    def accuracy_to_reward(self, acc):\n",
    "        return acc\n",
    "\n",
    "    def update_q_value_sequence(self, states, termination_reward):\n",
    "        self.update_q_value(states[-2], states[-1], termination_reward)\n",
    "        for i in reversed(range(len(states) - 2)):\n",
    "            \n",
    "            # TODO: q-learning update (set proper q-learning rate in cmdparser.py)\n",
    "            self.update_q_value(states[i], states[i+1], 0)\n",
    "\n",
    "            # TODO: modified update for shorter search schedules (doesn't use q-learning rate in computation)\n",
    "            # self.__update_q_value(states[i], states[i+1], termination_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}